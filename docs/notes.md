# Ideas on Visualization
1. A visual comparison between original images and the mix-up images generated by the mix-up augmentation.
2. Visualization on attention, provide insights into how the model focuses on different regions of the input
   - maybe use heatmap to visualize the attention?
3. Layer-wise feature visualization
4. Attention Evolution over Training
   - Potentially a series of heatmaps across epochs?
5. Prototype Visualization in Few-Shot Learning
   - Show how class prototypes evolve during training in your few-shot learning model.


# Dataset Description: Flowers102

The Flowers102 dataset is a comprehensive collection of flower images designed for image classification tasks. Key characteristics include:

1. **Size and Structure:**
   - Total images: 8,189
   - Classes: 102 different flower categories
   - Splits: Train (1,020 images), Validation (1,020 images), Test (6,149 images)

2. **Class Distribution:**
   - Train and Validation sets: Perfectly balanced with 10 images per class
   - Test set: Imbalanced, ranging from 20 to 238 images per class

3. **Features:**
   - High-quality images of flowers commonly found in the United Kingdom
   - Diverse in terms of scale, pose, and lighting conditions
   - Includes similar categories, challenging fine-grained classification

4. **Implications for Model Training:**
   - Balanced train/validation sets ideal for initial model training
   - Larger, imbalanced test set provides realistic performance evaluation
   - Suitable for developing robust flower classification models

5. **Baseline Model Performance:**

The following tables summarize the test accuracy (%) of various models trained on different dataset sizes:

| Model         | 100% Dataset | 50% Dataset | 25% Dataset | 10% Dataset |
|---------------|--------------|-------------|-------------|-------------|
| alexnet       | 73.96        | 59.89       | 43.72       | 24.27       |
| vgg16         | 81.43        | 66.10       | 48.60       | 26.71       |
| vgg19         | 76.71        | 66.33       | 45.80       | 27.52       |
| resnet18      | 82.06        | 63.31       | 42.03       | 16.94       |
| resnet50      | 85.98        | 71.76       | 49.32       | 27.52       |
| resnet101     | 84.99        | 69.23       | 46.06       | 24.76       |
| densenet121   | 86.94        | 70.43       | 48.28       | 27.36       |
| densenet201   | 88.99        | 73.52       | 47.17       | 22.15       |
| mobilenet_v2  | 86.19        | 73.91       | 49.25       | 19.06       |
| googlenet     | 81.92        | 63.21       | 37.35       | 2.93        |

Key observations:
- DenseNet201 achieves the highest accuracy (88.99%) when trained on the full dataset.
- Performance generally decreases as the dataset size is reduced, as expected.
- Some models (e.g., ResNet50, MobileNetV2) maintain relatively good performance even with reduced data.
- GoogleNet shows a dramatic drop in performance with smaller dataset sizes.

This baseline performance provides a good starting point for further model optimization and fine-tuning.


# Experimental Design Strategy

## Proposed Comparison Approach

### CopyBloomLens (Proposed Model)
- Train with **50% of training data**
- Leverages meta-learning/episodic approach
- Goal: Demonstrate efficient feature learning with limited data

### Baseline Models
- Train with **100% of training data**
- Standard training approach
- Uses full available dataset

## Rationale for Comparison Strategy
1. **Data Efficiency Demonstration:**
   - If BloomLens matches/exceeds baselines while using less data, it validates the meta-learning approach
   - Proves model's ability to learn efficiently from limited samples

2. **Experimental Fairness:**
   - Giving baselines more data strengthens the comparison
   - Makes any positive results more convincing

3. **Real-world Application:**
   - Demonstrates practical value of doing more with less data
   - Relevant for scenarios with limited data availability

## Proposed Experimental Structure
1. **Baseline Models:** Train with 100% data (current baseline results)
2. **BloomLens-50:** Train with 50% data
3. **BloomLens-25:** Train with 25% data

This structure will enable statements like:
- "Our model achieves X% accuracy using only 50% of the training data"
- "This matches/exceeds baseline models trained on the full dataset"

## Implementation Notes
- Keep existing baseline_comparison.py for full-data baseline evaluations
- Add reduced-data training option for BloomLens
- Run comprehensive experiments with different data reduction ratios
